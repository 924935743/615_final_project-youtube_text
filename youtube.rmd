---
title: "Youtube Trending Videos Analysis Report"
name: Yifeng Luo
output: pdf_document

---
#Introduction 
```{r setup, include=FALSE}
#set chunk option and load packages
knitr::opts_chunk$set(echo = FALSE)
pacman::p_load(
  "tidyverse",
  "data.table",
  "gridExtra",
  "sentimentr",
  "DT",
  "plotly",
  "readr",
  "jsonlite",
  "magrittr",
  "benford.analysis",
  "udpipe",
  "wordcloud",
  "shiny",
  "lubridate",
  "ggimage",
  "lattice")
```

#Data Resource
```{r,include=FALSE}
setwd("/Users/yifeng/Desktop/615_final project")
## read trending video data & category list for U.S.
us<-read_csv("USvideos.csv") 
us$country<-"US"

## clean outliers
us %<>% filter(comments_disabled=="False" & ratings_disabled=="False" & video_error_or_removed=="False")

#check NA 
sapply(us, function(x) sum(is.na(x)))
#547 discription -->NA 

#read reference of video category
us_refer<-data.frame(fromJSON(txt = "US_category_id.json",simplifyDataFrame = TRUE))
us_refer<-as.data.frame(cbind(us_refer$items.id,us_refer$items.snippet$title))
colnames(us_refer)<-c("category_id","category")

#change categoryID format
us_refer$category_id<-as.character(us_refer$category_id)
us_refer$category_id<-as.numeric(us_refer$category_id)

#join by ID
us<-left_join(us,us_refer,by="category_id")
```

```{r,include=FALSE}
## read Canadian data & category list
ca<-read_csv("CAvideos.csv")
ca$country<-"CA"

#clean outliers
ca %<>% filter(comments_disabled=="False" & ratings_disabled=="False" & video_error_or_removed=="False")

#check NA
sapply(ca, function(x) sum(is.na(x)))
#1271 discription -->NA 

#read reference of video category
ca_refer<-data.frame(fromJSON(txt = "CA_category_id.json",simplifyDataFrame = TRUE))
ca_refer<-as.data.frame(cbind(ca_refer$items.id,ca_refer$items.snippet$title))
colnames(ca_refer)<-c("category_id","category")

#change categoryID format
ca_refer$category_id<-as.character(ca_refer$category_id)
ca_refer$category_id<-as.numeric(ca_refer$category_id)

#join by ID
ca<-left_join(ca,us_refer,by="category_id")

```

```{r,include=FALSE}
## read British data & category list
gb<-read_csv("GBvideos.csv")
gb$country<-"GB"

#clean outliers
gb %<>% filter(comments_disabled=="False" & ratings_disabled=="False" & video_error_or_removed=="False")

#check NA
sapply(gb, function(x) sum(is.na(x)))
#589 discription -->NA 

#read reference of video category
gb_refer<-data.frame(fromJSON(txt = "GB_category_id.json",simplifyDataFrame = TRUE))
gb_refer<-as.data.frame(cbind(gb_refer$items.id,gb_refer$items.snippet$title))
colnames(gb_refer)<-c("category_id","category")

#change categoryID format
gb_refer$category_id<-as.character(gb_refer$category_id)
gb_refer$category_id<-as.numeric(gb_refer$category_id)

#join by ID
gb<-left_join(gb,us_refer,by="category_id")

```

```{r,include=FALSE}
## read india data & category list
ind<-read_csv("INvideos.csv")
ind$country<-"IN"

#clean outliers
#TRUE/FALSE is logical language
ind %<>% filter(comments_disabled==FALSE & ratings_disabled==FALSE & video_error_or_removed==FALSE)

#check NA
sapply(ind, function(x) sum(is.na(x)))
#528 discription -->NA 

#read reference of video category
ind_refer<-data.frame(fromJSON(txt = "IN_category_id.json",simplifyDataFrame = TRUE))
ind_refer<-as.data.frame(cbind(ind_refer$items.id,ind_refer$items.snippet$title))
colnames(ind_refer)<-c("category_id","category")

#change categoryID format
ind_refer$category_id<-as.character(ind_refer$category_id)
ind_refer$category_id<-as.numeric(ind_refer$category_id)

#join by ID
ind<-left_join(ind,us_refer,by="category_id")
```

```{r,include=FALSE}
#merge the dataset of 4 coutries as a big one
countries<-rbind(us,gb,ca,ind)

#change the data format 
#and calculate difference days from publishing date to trending date
countries$trending_date<-ydm(countries$trending_date)
countries$publish_time<-ymd(substr(countries$publish_time,1,10))
countries$dif_days <- countries$trending_date-countries$publish_time

#only analyse the trending data on the frist day for each video 
countries_first<- countries %>% group_by(country,video_id) %>% filter(views==head(views,1)) %>% distinct() %>% select(-c(category_id,publish_time,comments_disabled,ratings_disabled,video_error_or_removed))

#check NA
sapply(countries_first, function(x)sum(is.na(x)))
#1392 description NA
```
#Benford Analysis for Views
```{r}
country_benford<-benford(countries_first$views,number.of.digits = 2)
plot(country_benford)
country_benford
```

```{r}
country_suspect<-suspectsTable(country_benford)
country_suspect
suspect<-getSuspects(country_benford,countries_first,by="absolute.diff",how.many = 1)
suspect
```

#Visualization 
```{r}
#TOP VIEWS 
countries_first%>% arrange(-views) %>% filter(views>=38873543)%>% 
  mutate(image = paste0('<img width="80%" height="80%" src="', thumbnail_link , '"></img>')) %>% ungroup() %>% 
  select(image, country,category,title,views) %>% 
  datatable(class = "nowrap hover row-border", escape = FALSE, options = list(dom = 't',scrollX = TRUE, autoWidth = TRUE))
```


```{r}
rank2<-gb %>% filter(title=="Becky G, Natti Natasha - Sin Pijama (Official Video)") %>% select(views) 

rank2$inc<-NA
for (i in 1:21) { 
  rank2$inc[i]=rank2$views[i+1]-rank2$views[i]
}
rank2<-na.omit(rank2)
rank2<-rank2[-9,]
rank2$days<-c(1:19)

#summary(rank2$inc)
rank2$plot<-"1f601"
for (i in 1:19) {
  if(rank2$inc[i+1]>=rank2$inc[i]){
    rank2$plot[i+1]<-"1f601"
  }else{
    rank2$plot[i+1]<-"1f627"
  }
  
}

ggplot(rank2)+aes(days,inc)+geom_line()+geom_emoji(aes(x=days, y=inc,image=plot), size=.05)+ggtitle("The views trend for Runner-up video by trending days")+labs(y="Views")
```

```{r}
#category
countries_first %>% group_by(country,category) %>% mutate(n=1) %>% mutate(N=sum(n)) %>% 
ggplot()+aes(reorder(category,-N),fill=factor(category))+geom_bar(show.legend = F)+facet_grid(country~.,scales = "free_y")+theme(axis.text.x = element_text(angle = 15))+labs(x=NULL,y="Frequency")+ggtitle("The Frequency of Video Category in 4 Countries")
```

```{r}
#time difference
countries_first %>% filter(dif_days<16) %>% 
ggplot()+aes(factor(dif_days),fill=factor(dif_days))+geom_bar(show.legend = F)+facet_wrap(country~.,scales = "free_y")+labs(x=NULL,y="Frequency")+ggtitle("The Frequency of Time Gap between Video Publishing and Trending")
```


```{r}
#top channel
countries_first %>% group_by(channel_title) %>% transmute(n=1,N=sum(n)) %>% arrange(-N) %>% distinct(channel_title,N) %>% filter(N>190) %>% ggplot()+aes(reorder(channel_title,N),N,fill=channel_title)+geom_bar(stat = "identity")+geom_label(aes(label=N))+guides(fill="none")+theme(axis.text.x = element_text(hjust = 1))+ labs(title=" Top trending channels in 4 countries")+xlab(NULL)+ylab(NULL)+coord_flip()
```

```{r}
#tag-us
us_tag<-countries_first %>% ungroup() %>% filter(country=="US") %>% select(tags)
us_tag_split<-data.frame(tstrsplit(us_tag$tags, '\"|\"',fill = NA,fix=TRUE))
colnames(us_tag_split)<-c(1:69)
us_tag_split<- as.data.frame(t(us_tag_split))
us_tag_split %<>% gather(video,tags,V1:V6238,na.rm = TRUE) 
us_tag_split$tags<-tolower(us_tag_split$tags) 
us_tag_split %>% group_by(tags) %>% count() %>% arrange(-n) %>% filter(n>100) %>% with(wordcloud(tags,n,colors = brewer.pal(6,"Dark2"),random.order = F))
```

```{r}
#tag-CA
 ca_tag<-countries_first %>% ungroup() %>% filter(country=="CA") %>% select(tags)
 ca_tag_split<-data.frame(tstrsplit( ca_tag$tags, '\"|\"',fill = NA,fix=TRUE))
colnames( ca_tag_split)<-c(1:123)
 ca_tag_split<- as.data.frame(t(ca_tag_split))
 ca_tag_split %<>% gather(video,tags,V1:V23900,na.rm = TRUE) 
 ca_tag_split$tags<-tolower(ca_tag_split$tags) 
 ca_tag_split %>% group_by(tags) %>% count() %>% arrange(-n) %>% filter(n>250) %>% with(wordcloud(tags,n,colors = brewer.pal(6,"Dark2"),random.order = F))
```

```{r}
#tag-IN
in_tag<-countries_first %>% ungroup() %>% filter(country=="IN") %>% select(tags)
in_tag_split<-data.frame(tstrsplit(in_tag$tags, '\"|\"',fill = NA,fix=TRUE))
colnames(in_tag_split)<-c(1:71)
in_tag_split<- as.data.frame(t(in_tag_split))
in_tag_split %<>% gather(video,tags,V1:V15716,na.rm = TRUE) %>% mutate(tags = str_replace_all(tags,'"',""))
in_tag_split$tags<-tolower(in_tag_split$tags) 
in_tag_split %>% group_by(tags) %>% count() %>% arrange(-n) %>% mutate(N=n-300)%>%  filter(N>0) %>% with(wordcloud(tags,N,colors = brewer.pal(6,"Dark2"),random.order = F))

```

```{r}
#tag-gb
gb_tag<-countries_first %>% ungroup() %>% filter(country=="GB") %>% select(tags)
gb_tag_split<-data.frame(tstrsplit(gb_tag$tags, '\"|\"',fill = NA,fix=TRUE))
colnames(gb_tag_split)<-c(1:78)
gb_tag_split<- as.data.frame(t(gb_tag_split))
gb_tag_split %<>% gather(video,tags,V1:V3216,na.rm = TRUE)
gb_tag_split$tags<-tolower(gb_tag_split$tags) 
gb_tag_split %>% group_by(tags) %>% count() %>% arrange(-n) %>% filter(n>60) %>% with(wordcloud(tags,n,colors = brewer.pal(6,"Dark2"),random.order = F))
```


```{r,include=FALSE}
if (file.exists("english-ud-2.0-170801.udpipe")) 
  ud_model <- udpipe_load_model(file = "english-ud-2.0-170801.udpipe") else {
    ud_model <- udpipe_download_model(language = "english")
    ud_model <- udpipe_load_model(ud_model$file_model)
}
```

```{r}
us_desc<-countries_first %>% ungroup() %>%  filter(country=="US") %>% select(description) 
us_udpipe <- udpipe_annotate(ud_model, us_desc$description)
us_udpipe<-data.frame(us_udpipe)
```


```{r,warning=FALSE,error=FALSE}
us_udpipe$phrase_tag <- as_phrasemachine(us_udpipe$upos, type = "upos")

stats_us <- keywords_phrases(x = us_udpipe$phrase_tag, term = tolower(us_udpipe$token), 
                          pattern = "(A|N)*N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, detailed = FALSE)


stats_us <- subset(stats_us, ngram > 1 & freq > 3)
stats_us$key <- factor(stats_us$keyword, levels = rev(stats_us$keyword))
barchart(key ~ freq, data = head(stats_us, 20), col = "magenta", 
         main = "Keywords - simple noun phrases", xlab = "Frequency")

stats_us %>% top_n(100) %>% with(wordcloud(key,freq,colors = brewer.pal(6,"Dark2"),random.order = F))
```

```{r}
ca_desc<-countries_first %>% ungroup() %>%  filter(country=="CA") %>% select(description) 
ca_udpipe <- udpipe_annotate(ud_model, ca_desc$description)
ca_udpipe<-data.frame(ca_udpipe)
```


```{r}
ca_udpipe$phrase_tag <- as_phrasemachine(ca_udpipe$upos, type = "upos")

stats_ca <- keywords_phrases(x = ca_udpipe$phrase_tag, term = tolower(ca_udpipe$token), 
                          pattern = "(A|N)*N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, detailed = FALSE)


stats_ca <- subset(stats_ca, ngram > 1 & freq > 3)
stats_ca$key <- factor(stats_ca$keyword, levels = rev(stats_ca$keyword))
barchart(key ~ freq, data = head(stats_ca, 20), col = "magenta", 
         main = "Keywords - simple noun phrases", xlab = "Frequency")

stats_ca %>% top_n(100) %>% with(wordcloud(key,freq,colors = brewer.pal(6,"Dark2"),random.order = F))
```


```{r}
gb_desc<-countries_first %>% ungroup() %>%  filter(country=="GB") %>% select(description) 
gb_udpipe <- udpipe_annotate(ud_model, gb_desc$description)
gb_udpipe<-data.frame(gb_udpipe)
```

```{r}
gb_udpipe$phrase_tag <- as_phrasemachine(gb_udpipe$upos, type = "upos")

stats_gb <- keywords_phrases(x = gb_udpipe$phrase_tag, term = tolower(gb_udpipe$token), 
                          pattern = "(A|N)*N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, detailed = FALSE)


stats_gb <- subset(stats_gb, ngram > 1 & freq > 3)
stats_gb$key <- factor(stats_gb$keyword, levels = rev(stats_gb$keyword))
barchart(key ~ freq, data = head(stats_gb, 20), col = "magenta", 
         main = "Keywords - simple noun phrases", xlab = "Frequency")

stats_gb %>% top_n(100) %>% with(wordcloud(key,freq,colors = brewer.pal(6,"Dark2"),random.order = F))
```

```{r}
in_desc<-countries_first %>% ungroup() %>%  filter(country=="IN") %>% select(description) 
in_udpipe <- udpipe_annotate(ud_model, in_desc$description)
in_udpipe<-data.frame(in_udpipe)
```

```{r}
in_udpipe$phrase_tag <- as_phrasemachine(in_udpipe$upos, type = "upos")

stats_in <- keywords_phrases(x = in_udpipe$phrase_tag, term = tolower(in_udpipe$token), 
                          pattern = "(A|N)*N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, detailed = FALSE)


stats_in <- subset(stats_in, ngram > 1 & freq > 3)
stats_in$key <- factor(stats_in$keyword, levels = rev(stats_in$keyword))
barchart(key ~ freq, data = head(stats_in, 20), col = "magenta", 
         main = "Keywords - simple noun phrases", xlab = "Frequency")

stats_in %>% top_n(100) %>% with(wordcloud(key,freq,colors = brewer.pal(6,"Dark2"),random.order = F))

```


```{r}
#sentiment anlysis
countries_first %>% ungroup() %>% select(country,description) %>% unnest_tokens(word,description) %>%  mutate(word = str_extract(word, "[a-z']+")) %>% anti_join(stop_words) %>% inner_join(get_sentiments("bing")) %>% ggplot()+aes(sentiment,fill=sentiment)+geom_bar()+facet_wrap(country~.,scales = "free_y")+ggtitle("Negative vs. Positive")


sentiment_desc<-countries_first %>% ungroup() %>% select(country,description) %>% unnest_tokens(word,description) %>%  mutate(word = str_extract(word, "[a-z']+")) %>% anti_join(stop_words) %>% inner_join(get_sentiments("bing")) %>% group_by(country,sentiment) %>% count() %>%  ggplot()+aes(country,n)+geom_bar(aes(fill=sentiment),stat = "identity",position = "dodge")+ggtitle("Negative vs. Positive")

ggplotly(sentiment_desc)
```

```{r}
#sentiment score 

#us
us_desc %<>% filter(!is.na(description))
us_desc <-sentiment(us_desc$description) 
us_desc$country<-"US"
#ca
ca_desc %<>% filter(!is.na(description))
ca_desc<-sentiment(ca_desc$description)
ca_desc$country<-"CA"
#in
in_desc %<>% filter(!is.na(description))
in_desc<-sentiment(in_desc$description)
in_desc$country<-"IN"
#gb
gb_desc %<>% filter(!is.na(description))
gb_desc<-sentiment(gb_desc$description)
gb_desc$country<-"GB"


#facet
sentiment_score<-rbind(us_desc,ca_desc,in_desc,gb_desc)

sentiment_score %>% group_by(country,element_id) %>% mutate(sentiment=sum(sentiment)) %>% select(country,element_id,sentiment) %>% distinct() %>% ggplot()+aes(sentiment,fill="red")+geom_histogram(show.legend = F,bins = 40)+facet_wrap(country~.,scales = "free_y")+ggtitle("The Distribution of Description Score in these 4 Countries")+geom_vline(xintercept = 0, color = "black", size = 1, alpha = 0.6, linetype = "longdash") +coord_cartesian(xlim = c(-3, 7))


sentiment_score %>% group_by(country,element_id) %>% mutate(sentiment=sum(sentiment)) %>% select(country,element_id,sentiment) %>% distinct() %>% ggplot()+aes(sentiment,color=factor(country))+geom_density()+geom_vline(xintercept = 0, color = "black", size = 1, alpha = 0.6, linetype = "longdash")
```
